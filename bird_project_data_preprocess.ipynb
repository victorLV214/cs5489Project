{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f8d6dd2",
   "metadata": {
    "papermill": {
     "duration": 0.002284,
     "end_time": "2025-04-14T13:45:53.973091",
     "exception": false,
     "start_time": "2025-04-14T13:45:53.970807",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Bird Project Data Preprocessing (In Kaggle Env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab329620",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import glob\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "import torchaudio.transforms as AT\n",
    "from scipy import signal\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "torch.set_num_threads(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1634828",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T13:45:53.978863Z",
     "iopub.status.busy": "2025-04-14T13:45:53.978446Z",
     "iopub.status.idle": "2025-04-14T13:46:16.278049Z",
     "shell.execute_reply": "2025-04-14T13:46:16.273022Z"
    },
    "papermill": {
     "duration": 22.306507,
     "end_time": "2025-04-14T13:46:16.281638",
     "exception": false,
     "start_time": "2025-04-14T13:45:53.975131",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MyAudioConfig:\n",
    "    def __init__(self):\n",
    "\n",
    "        self.sample_rate = 32000\n",
    "        # Audio window length, in seconds\n",
    "        self.window_size = 5\n",
    "\n",
    "        # Mel spectrum parameters\n",
    "        self.n_fft = 2048  # FFT window size (frequency domain resolution)\n",
    "        self.hop_length = (\n",
    "            512  # How many points are used to perform a window (time resolution)\n",
    "        )\n",
    "        self.n_mels = 128  # The number of Mel bins output, the larger the number, the higher the frequency resolution\n",
    "        self.fmin = 20  # Minimum frequency range\n",
    "        self.fmax = 16000  # Maximum frequency range (half the sampling rate)\n",
    "\n",
    "        self.power = 2.0  # Power spectrum index, 2.0 represents power spectrum, 1.0 represents amplitude spectrum\n",
    "\n",
    "        self.target_shape = (256, 128)\n",
    "\n",
    "        # Noise processing parameters\n",
    "        self.apply_noise_reduction = True  # Whether to enable noise reduction\n",
    "        self.noise_reduction_strength = 0.1  # Noise reduction strength: the ratio of the original signal + the noise reduction signal\n",
    "\n",
    "        # Audio normalization, remove DC offset and normalize amplitude\n",
    "        self.apply_normalization = True\n",
    "\n",
    "        # Spectrum contrast enhancement\n",
    "        self.apply_spec_contrast = True\n",
    "        self.contrast_factor = 0.15\n",
    "\n",
    "        # SpecAugment\n",
    "        self.use_spec_augment = False\n",
    "        self.freq_mask_param = 20\n",
    "        self.time_mask_param = 30\n",
    "        self.freq_mask_count = 1\n",
    "        self.time_mask_count = 1\n",
    "\n",
    "        # Small constant for numerical stability\n",
    "        self.eps = 1e-6\n",
    "\n",
    "\n",
    "class MyAudioPipeline:\n",
    "    \"\"\"\n",
    "    A pipeline class that converts raw audio waveform into\n",
    "    a final Mel-spectrogram, mirroring the structure of the\n",
    "    original big code. This includes:\n",
    "    - Noise reduction\n",
    "    - Normalization\n",
    "    - Mel-spectrogram\n",
    "    - dB scaling + minmax normalization\n",
    "    - (Optional) Contrast enhancement\n",
    "    - (Optional) SpecAugment\n",
    "    - Resizing to target shape\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cfg: MyAudioConfig):\n",
    "        self.cfg = cfg\n",
    "\n",
    "    def reduce_noise(self, audio_data: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Apply simple median-filter-based denoising, then mix\n",
    "        original signal and denoised signal by noise_reduction_strength.\n",
    "        \"\"\"\n",
    "        if not self.cfg.apply_noise_reduction:\n",
    "            return audio_data\n",
    "\n",
    "        # Median filter\n",
    "        window_size = 5\n",
    "        audio_denoised = signal.medfilt(audio_data, window_size)\n",
    "\n",
    "        # Mix\n",
    "        alpha = self.cfg.noise_reduction_strength\n",
    "        return (1 - alpha) * audio_data + alpha * audio_denoised\n",
    "\n",
    "    def normalize_audio(self, audio_data: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Remove DC offset and scale to [-1, 1].\n",
    "        \"\"\"\n",
    "        if not self.cfg.apply_normalization:\n",
    "            return audio_data\n",
    "\n",
    "        mean_val = np.mean(audio_data)\n",
    "        audio_data = audio_data - mean_val\n",
    "\n",
    "        max_amp = np.max(np.abs(audio_data))\n",
    "        if max_amp > 0:\n",
    "            audio_data = audio_data / max_amp\n",
    "\n",
    "        return audio_data\n",
    "\n",
    "    def enhance_spectrogram_contrast(\n",
    "        self, spec: np.ndarray, factor: float = 0.15\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Enhance spectrogram contrast. We shift values away from the mean,\n",
    "        then clip to [0,1].\n",
    "        \"\"\"\n",
    "        mean_val = np.mean(spec)\n",
    "        enhanced = mean_val + (spec - mean_val) * (1 + factor)\n",
    "        return np.clip(enhanced, 0, 1)\n",
    "\n",
    "    def apply_spec_augment(self, spec: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        SpecAugment: randomly mask along frequency & time axes.\n",
    "        \"\"\"\n",
    "        if not self.cfg.use_spec_augment:\n",
    "            return spec\n",
    "\n",
    "        augmented = spec.copy()\n",
    "\n",
    "        # Frequency mask\n",
    "        for _ in range(self.cfg.freq_mask_count):\n",
    "            f = np.random.randint(0, self.cfg.freq_mask_param)\n",
    "            f0 = np.random.randint(0, augmented.shape[0] - f)\n",
    "            augmented[f0 : f0 + f, :] = 0\n",
    "\n",
    "        # Time mask\n",
    "        for _ in range(self.cfg.time_mask_count):\n",
    "            t = np.random.randint(0, self.cfg.time_mask_param)\n",
    "            t0 = np.random.randint(0, augmented.shape[1] - t)\n",
    "            augmented[:, t0 : t0 + t] = 0\n",
    "\n",
    "        return augmented\n",
    "\n",
    "    def audio_to_melspec(self, audio_data: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Core function that:\n",
    "         1) Pad or trim to self.cfg.window_size\n",
    "         2) reduce_noise -> normalize_audio\n",
    "         3) librosa.feature.melspectrogram\n",
    "         4) power_to_db -> [0,1] minmax\n",
    "         5) (Optional) enhance contrast\n",
    "         6) (Optional) spec augment\n",
    "         7) resize\n",
    "        \"\"\"\n",
    "        # 1) Pad or trim to exactly (sample_rate * window_size) samples\n",
    "        required_samples = self.cfg.sample_rate * self.cfg.window_size\n",
    "        if len(audio_data) < required_samples:\n",
    "            audio_data = np.pad(\n",
    "                audio_data, (0, required_samples - len(audio_data)), mode=\"constant\"\n",
    "            )\n",
    "        elif len(audio_data) > required_samples:\n",
    "            audio_data = audio_data[:required_samples]\n",
    "\n",
    "        # 2) Denoise & Normalize\n",
    "        audio_data = self.reduce_noise(audio_data)\n",
    "        audio_data = self.normalize_audio(audio_data)\n",
    "\n",
    "        # 3) Mel-spectrogram\n",
    "        mel = librosa.feature.melspectrogram(\n",
    "            y=audio_data,\n",
    "            sr=self.cfg.sample_rate,\n",
    "            n_fft=self.cfg.n_fft,\n",
    "            hop_length=self.cfg.hop_length,\n",
    "            n_mels=self.cfg.n_mels,\n",
    "            fmin=self.cfg.fmin,\n",
    "            fmax=self.cfg.fmax,\n",
    "            power=self.cfg.power,\n",
    "        )\n",
    "\n",
    "        # 4) Convert to dB, then min-max to [0,1]\n",
    "        mel_db = librosa.power_to_db(mel, ref=np.max)\n",
    "        db_min, db_max = mel_db.min(), mel_db.max()\n",
    "        mel_norm = (mel_db - db_min) / (db_max - db_min + self.cfg.eps)\n",
    "\n",
    "        # 5) Enhance contrast if needed\n",
    "        if self.cfg.apply_spec_contrast:\n",
    "            mel_norm = self.enhance_spectrogram_contrast(\n",
    "                mel_norm, self.cfg.contrast_factor\n",
    "            )\n",
    "\n",
    "        # 6) SpecAugment (time/freq mask)\n",
    "        mel_aug = self.apply_spec_augment(mel_norm)\n",
    "\n",
    "        # 7) Resize\n",
    "        # if target_shape=(256,128) means (width=256, height=128)\n",
    "        # but mel shape is (n_mels, time_frames) -> (128, ???)\n",
    "        # So we do: cv2.resize(mel_aug, (width, height))\n",
    "        if self.cfg.target_shape is not None:\n",
    "            mel_aug = cv2.resize(\n",
    "                mel_aug, self.cfg.target_shape, interpolation=cv2.INTER_LINEAR\n",
    "            )\n",
    "\n",
    "        # Return float32 array\n",
    "        return mel_aug.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9376aadc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T13:46:16.295296Z",
     "iopub.status.busy": "2025-04-14T13:46:16.294561Z",
     "iopub.status.idle": "2025-04-14T14:25:16.946970Z",
     "shell.execute_reply": "2025-04-14T14:25:16.945430Z"
    },
    "papermill": {
     "duration": 2341.46309,
     "end_time": "2025-04-14T14:25:17.752505",
     "exception": false,
     "start_time": "2025-04-14T13:46:16.289415",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28564 ogg files under /kaggle/input/birdclef-2025/train_audio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28564/28564 [38:08<00:00, 12.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved processed data to train_data.npy\n"
     ]
    }
   ],
   "source": [
    "cfg = MyAudioConfig()\n",
    "pipeline = MyAudioPipeline(cfg)\n",
    "\n",
    "train_audio_dir = '/kaggle/input/birdclef-2025/train_audio'\n",
    "output_file = 'train_data.npy'\n",
    "\n",
    "data_dict = {}\n",
    "\n",
    "ogg_files = glob.glob(os.path.join(train_audio_dir, '**', '*.ogg'), recursive=True)\n",
    "print(f\"Found {len(ogg_files)} ogg files under {train_audio_dir}\")\n",
    "\n",
    "for oggfile in tqdm(ogg_files):\n",
    "    label_dir = os.path.basename(os.path.dirname(oggfile))\n",
    "    label = label_dir \n",
    "    filename = os.path.basename(oggfile)\n",
    "    \n",
    "    y, sr = librosa.load(oggfile, sr=cfg.sample_rate, mono=True)\n",
    "    \n",
    "    mel_spec = pipeline.audio_to_melspec(y)\n",
    "    \n",
    "    file_id = f\"{label}_{filename}\"\n",
    "    data_dict[file_id] = {\n",
    "        \"data\": mel_spec,\n",
    "        \"label\": label\n",
    "    }\n",
    "\n",
    "np.save(output_file, data_dict, allow_pickle=True)\n",
    "print(f\"Saved processed data to {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 11361821,
     "sourceId": 91844,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2372.55083,
   "end_time": "2025-04-14T14:25:21.536883",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-04-14T13:45:48.986053",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
