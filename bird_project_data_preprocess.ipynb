{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f8d6dd2",
   "metadata": {
    "papermill": {
     "duration": 0.002284,
     "end_time": "2025-04-14T13:45:53.973091",
     "exception": false,
     "start_time": "2025-04-14T13:45:53.970807",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Bird Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1634828",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T13:45:53.978863Z",
     "iopub.status.busy": "2025-04-14T13:45:53.978446Z",
     "iopub.status.idle": "2025-04-14T13:46:16.278049Z",
     "shell.execute_reply": "2025-04-14T13:46:16.273022Z"
    },
    "papermill": {
     "duration": 22.306507,
     "end_time": "2025-04-14T13:46:16.281638",
     "exception": false,
     "start_time": "2025-04-14T13:45:53.975131",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import glob\n",
    "import librosa\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import sys\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import timm\n",
    "import torchaudio\n",
    "import torchaudio.transforms as AT\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "from scipy import signal\n",
    "import gc\n",
    "import warnings\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ''\n",
    "torch.set_num_threads(4) \n",
    "\n",
    "class MyAudioConfig:\n",
    "    def __init__(self):        \n",
    "        self.sample_rate = 32000\n",
    "        # 音频窗口长度，单位秒\n",
    "        self.window_size = 5\n",
    "\n",
    "        # Mel 频谱图参数\n",
    "        self.n_fft = 2048               # FFT 窗口大小（频域分辨率）\n",
    "        self.hop_length = 512           # 每隔多少点进行一次窗口（时间分辨率）\n",
    "        self.n_mels = 128               # 输出的 Mel bin 数量，越大频率分辨率越高\n",
    "        self.fmin = 20                  # 最低频率范围\n",
    "        self.fmax = 16000               # 最高频率范围（采样率一半）\n",
    "\n",
    "        self.power = 2.0                # 功率谱指数，2.0 表示功率谱，1.0 表示幅度谱\n",
    "\n",
    "        # 输出图片尺寸（输入到模型前 resize 成统一大小）\n",
    "        self.target_shape = (256, 128)  # 宽度 256，高度 128\n",
    "\n",
    "        # 噪声处理参数\n",
    "        self.apply_noise_reduction = True  # 是否开启降噪\n",
    "        self.noise_reduction_strength = 0.1  # 降噪强度：原始信号 + 降噪信号混合比例\n",
    "\n",
    "        # 音频归一化，去除直流偏移并归一化振幅\n",
    "        self.apply_normalization = True\n",
    "\n",
    "        # 频谱图对比度增强\n",
    "        self.apply_spec_contrast = True\n",
    "        self.contrast_factor = 0.15  # 增强力度\n",
    "\n",
    "        # SpecAugment（训练增强，防止过拟合）\n",
    "        self.use_spec_augment = False\n",
    "        self.freq_mask_param = 20\n",
    "        self.time_mask_param = 30\n",
    "        self.freq_mask_count = 1\n",
    "        self.time_mask_count = 1\n",
    "\n",
    "        # 数值稳定性的小常数\n",
    "        self.eps = 1e-6\n",
    "\n",
    "\n",
    "class MyAudioPipeline:\n",
    "    \"\"\"\n",
    "    A pipeline class that converts raw audio waveform into\n",
    "    a final Mel-spectrogram, mirroring the structure of the \n",
    "    original big code. This includes:\n",
    "    - Noise reduction\n",
    "    - Normalization\n",
    "    - Mel-spectrogram\n",
    "    - dB scaling + minmax normalization\n",
    "    - (Optional) Contrast enhancement\n",
    "    - (Optional) SpecAugment\n",
    "    - Resizing to target shape\n",
    "    \"\"\"\n",
    "    def __init__(self, cfg: MyAudioConfig):\n",
    "        self.cfg = cfg\n",
    "\n",
    "    def reduce_noise(self, audio_data: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Apply simple median-filter-based denoising, then mix\n",
    "        original signal and denoised signal by noise_reduction_strength.\n",
    "        \"\"\"\n",
    "        if not self.cfg.apply_noise_reduction:\n",
    "            return audio_data\n",
    "        \n",
    "        # Median filter\n",
    "        window_size = 5\n",
    "        audio_denoised = signal.medfilt(audio_data, window_size)\n",
    "        \n",
    "        # Mix\n",
    "        alpha = self.cfg.noise_reduction_strength\n",
    "        return (1 - alpha) * audio_data + alpha * audio_denoised\n",
    "\n",
    "    def normalize_audio(self, audio_data: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Remove DC offset and scale to [-1, 1].\n",
    "        \"\"\"\n",
    "        if not self.cfg.apply_normalization:\n",
    "            return audio_data\n",
    "        \n",
    "        mean_val = np.mean(audio_data)\n",
    "        audio_data = audio_data - mean_val\n",
    "        \n",
    "        max_amp = np.max(np.abs(audio_data))\n",
    "        if max_amp > 0:\n",
    "            audio_data = audio_data / max_amp\n",
    "        \n",
    "        return audio_data\n",
    "\n",
    "    def enhance_spectrogram_contrast(self, spec: np.ndarray, factor: float = 0.15) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Enhance spectrogram contrast. We shift values away from the mean,\n",
    "        then clip to [0,1].\n",
    "        \"\"\"\n",
    "        mean_val = np.mean(spec)\n",
    "        enhanced = mean_val + (spec - mean_val) * (1 + factor)\n",
    "        return np.clip(enhanced, 0, 1)\n",
    "\n",
    "    def apply_spec_augment(self, spec: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        SpecAugment: randomly mask along frequency & time axes.\n",
    "        \"\"\"\n",
    "        if not self.cfg.use_spec_augment:\n",
    "            return spec\n",
    "        \n",
    "        augmented = spec.copy()\n",
    "        \n",
    "        # Frequency mask\n",
    "        for _ in range(self.cfg.freq_mask_count):\n",
    "            f = np.random.randint(0, self.cfg.freq_mask_param)\n",
    "            f0 = np.random.randint(0, augmented.shape[0] - f)\n",
    "            augmented[f0:f0+f, :] = 0\n",
    "        \n",
    "        # Time mask\n",
    "        for _ in range(self.cfg.time_mask_count):\n",
    "            t = np.random.randint(0, self.cfg.time_mask_param)\n",
    "            t0 = np.random.randint(0, augmented.shape[1] - t)\n",
    "            augmented[:, t0:t0+t] = 0\n",
    "        \n",
    "        return augmented\n",
    "    \n",
    "    def audio_to_melspec(self, audio_data: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Core function that:\n",
    "         1) Pad or trim to self.cfg.window_size\n",
    "         2) reduce_noise -> normalize_audio\n",
    "         3) librosa.feature.melspectrogram\n",
    "         4) power_to_db -> [0,1] minmax\n",
    "         5) (Optional) enhance contrast\n",
    "         6) (Optional) spec augment\n",
    "         7) resize\n",
    "        \"\"\"\n",
    "        # 1) Pad or trim to exactly (sample_rate * window_size) samples\n",
    "        required_samples = self.cfg.sample_rate * self.cfg.window_size\n",
    "        if len(audio_data) < required_samples:\n",
    "            audio_data = np.pad(audio_data, (0, required_samples - len(audio_data)), mode='constant')\n",
    "        elif len(audio_data) > required_samples:\n",
    "            audio_data = audio_data[:required_samples]\n",
    "        \n",
    "        # 2) Denoise & Normalize\n",
    "        audio_data = self.reduce_noise(audio_data)\n",
    "        audio_data = self.normalize_audio(audio_data)\n",
    "        \n",
    "        # 3) Mel-spectrogram\n",
    "        mel = librosa.feature.melspectrogram(\n",
    "            y=audio_data,\n",
    "            sr=self.cfg.sample_rate,\n",
    "            n_fft=self.cfg.n_fft,\n",
    "            hop_length=self.cfg.hop_length,\n",
    "            n_mels=self.cfg.n_mels,\n",
    "            fmin=self.cfg.fmin,\n",
    "            fmax=self.cfg.fmax,\n",
    "            power=self.cfg.power\n",
    "        )\n",
    "        \n",
    "        # 4) Convert to dB, then min-max to [0,1]\n",
    "        mel_db = librosa.power_to_db(mel, ref=np.max)\n",
    "        db_min, db_max = mel_db.min(), mel_db.max()\n",
    "        mel_norm = (mel_db - db_min) / (db_max - db_min + self.cfg.eps)\n",
    "        \n",
    "        # 5) Enhance contrast if needed\n",
    "        if self.cfg.apply_spec_contrast:\n",
    "            mel_norm = self.enhance_spectrogram_contrast(mel_norm, self.cfg.contrast_factor)\n",
    "        \n",
    "        # 6) SpecAugment (time/freq mask)\n",
    "        mel_aug = self.apply_spec_augment(mel_norm)\n",
    "        \n",
    "        # 7) Resize\n",
    "        # if target_shape=(256,128) means (width=256, height=128)\n",
    "        # but mel shape is (n_mels, time_frames) -> (128, ???)\n",
    "        # So we do: cv2.resize(mel_aug, (width, height))\n",
    "        if self.cfg.target_shape is not None:\n",
    "            mel_aug = cv2.resize(mel_aug, self.cfg.target_shape, interpolation=cv2.INTER_LINEAR)\n",
    "        \n",
    "        # Return float32 array\n",
    "        return mel_aug.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9376aadc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T13:46:16.295296Z",
     "iopub.status.busy": "2025-04-14T13:46:16.294561Z",
     "iopub.status.idle": "2025-04-14T14:25:16.946970Z",
     "shell.execute_reply": "2025-04-14T14:25:16.945430Z"
    },
    "papermill": {
     "duration": 2341.46309,
     "end_time": "2025-04-14T14:25:17.752505",
     "exception": false,
     "start_time": "2025-04-14T13:46:16.289415",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28564 ogg files under /kaggle/input/birdclef-2025/train_audio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28564/28564 [38:08<00:00, 12.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved processed data to train_data.npy\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import librosa\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "cfg = MyAudioConfig()\n",
    "pipeline = MyAudioPipeline(cfg)\n",
    "\n",
    "train_audio_dir = '/kaggle/input/birdclef-2025/train_audio'\n",
    "output_file = 'train_data.npy'\n",
    "\n",
    "data_dict = {}\n",
    "\n",
    "ogg_files = glob.glob(os.path.join(train_audio_dir, '**', '*.ogg'), recursive=True)\n",
    "print(f\"Found {len(ogg_files)} ogg files under {train_audio_dir}\")\n",
    "\n",
    "for oggfile in tqdm(ogg_files):\n",
    "    label_dir = os.path.basename(os.path.dirname(oggfile))\n",
    "    label = label_dir \n",
    "    filename = os.path.basename(oggfile)\n",
    "    \n",
    "    y, sr = librosa.load(oggfile, sr=cfg.sample_rate, mono=True)\n",
    "    \n",
    "    mel_spec = pipeline.audio_to_melspec(y)\n",
    "    \n",
    "    file_id = f\"{label}_{filename}\"\n",
    "    data_dict[file_id] = {\n",
    "        \"data\": mel_spec,\n",
    "        \"label\": label\n",
    "    }\n",
    "\n",
    "np.save(output_file, data_dict, allow_pickle=True)\n",
    "print(f\"Saved processed data to {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 11361821,
     "sourceId": 91844,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2372.55083,
   "end_time": "2025-04-14T14:25:21.536883",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-04-14T13:45:48.986053",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
