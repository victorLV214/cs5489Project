{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traditional machine learning algorithms to classify audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T18:47:10.765240Z",
     "iopub.status.busy": "2025-04-14T18:47:10.764930Z",
     "iopub.status.idle": "2025-04-14T18:47:12.036803Z",
     "shell.execute_reply": "2025-04-14T18:47:12.036256Z",
     "shell.execute_reply.started": "2025-04-14T18:47:10.765219Z"
    }
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import glob\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "import joblib\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T18:47:35.194429Z",
     "iopub.status.busy": "2025-04-14T18:47:35.193627Z",
     "iopub.status.idle": "2025-04-14T18:47:57.295156Z",
     "shell.execute_reply": "2025-04-14T18:47:57.294473Z",
     "shell.execute_reply.started": "2025-04-14T18:47:35.194405Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_array shape: (28564, 32768)\n",
      "y_array shape: (28564,)\n",
      "Example label: greani1\n"
     ]
    }
   ],
   "source": [
    "# data_dict = np.load('/kaggle/input/train-data-npy/train_data.npy', allow_pickle=True).item()\n",
    "data_dict = np.load('dataset/train_data.npy', allow_pickle=True).item()\n",
    "\n",
    "X_list = []\n",
    "y_list = []\n",
    "for fid, content in data_dict.items():\n",
    "    mel_2d = content['data']  # shape (128,256)\n",
    "    label_str = content['label']\n",
    "\n",
    "    # Flatten from (128,256) => (32768,)\n",
    "    mel_1d = mel_2d.flatten()\n",
    "\n",
    "    X_list.append(mel_1d)\n",
    "    y_list.append(label_str)\n",
    "\n",
    "X_array = np.array(X_list)  # shape (N, 128*256)\n",
    "y_array = np.array(y_list)  # shape (N,)\n",
    "\n",
    "print(\"X_array shape:\", X_array.shape)   # e.g. (N, 32768)\n",
    "print(\"y_array shape:\", y_array.shape)   # e.g. (N,)\n",
    "print(\"Example label:\", y_array[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T18:47:57.296581Z",
     "iopub.status.busy": "2025-04-14T18:47:57.296090Z",
     "iopub.status.idle": "2025-04-14T18:47:59.195721Z",
     "shell.execute_reply": "2025-04-14T18:47:59.194989Z",
     "shell.execute_reply.started": "2025-04-14T18:47:57.296559Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_raw, X_test_raw, y_train_raw, y_test_raw = train_test_split(\n",
    "    X_array, y_array, stratify=y_array, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 标签编码\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(y_train_raw)\n",
    "y_test = le.transform(y_test_raw)\n",
    "\n",
    "# PCA 处理（注意这里对 train 做 fit，再 transform test）\n",
    "pca = PCA(n_components=34)\n",
    "X_train_pca = pca.fit_transform(X_train_raw)\n",
    "X_test_pca = pca.transform(X_test_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting xgboost\n",
      "  Downloading xgboost-3.0.0-py3-none-win_amd64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (from xgboost) (1.26.4)\n",
      "Requirement already satisfied: scipy in c:\\programdata\\anaconda3\\lib\\site-packages (from xgboost) (1.13.1)\n",
      "Downloading xgboost-3.0.0-py3-none-win_amd64.whl (150.0 MB)\n",
      "   ---------------------------------------- 0.0/150.0 MB ? eta -:--:--\n",
      "    --------------------------------------- 2.9/150.0 MB 21.0 MB/s eta 0:00:08\n",
      "   - -------------------------------------- 6.8/150.0 MB 18.3 MB/s eta 0:00:08\n",
      "   --- ------------------------------------ 12.3/150.0 MB 20.3 MB/s eta 0:00:07\n",
      "   ---- ----------------------------------- 17.0/150.0 MB 21.5 MB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 21.2/150.0 MB 20.6 MB/s eta 0:00:07\n",
      "   ------ --------------------------------- 25.2/150.0 MB 20.4 MB/s eta 0:00:07\n",
      "   ------- -------------------------------- 29.1/150.0 MB 20.3 MB/s eta 0:00:06\n",
      "   --------- ------------------------------ 33.8/150.0 MB 20.4 MB/s eta 0:00:06\n",
      "   ---------- ----------------------------- 39.1/150.0 MB 21.2 MB/s eta 0:00:06\n",
      "   ----------- ---------------------------- 43.5/150.0 MB 21.1 MB/s eta 0:00:06\n",
      "   ------------ --------------------------- 47.4/150.0 MB 21.0 MB/s eta 0:00:05\n",
      "   ------------- -------------------------- 51.6/150.0 MB 21.1 MB/s eta 0:00:05\n",
      "   -------------- ------------------------- 56.1/150.0 MB 21.0 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 59.0/150.0 MB 20.5 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 61.6/150.0 MB 20.0 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 64.5/150.0 MB 19.7 MB/s eta 0:00:05\n",
      "   ------------------ --------------------- 67.6/150.0 MB 19.4 MB/s eta 0:00:05\n",
      "   ------------------- -------------------- 71.6/150.0 MB 19.3 MB/s eta 0:00:05\n",
      "   -------------------- ------------------- 75.8/150.0 MB 19.4 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 80.7/150.0 MB 19.7 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 84.9/150.0 MB 19.6 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 89.7/150.0 MB 19.8 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 96.5/150.0 MB 20.5 MB/s eta 0:00:03\n",
      "   --------------------------- ----------- 104.1/150.0 MB 21.0 MB/s eta 0:00:03\n",
      "   ----------------------------- --------- 111.7/150.0 MB 21.7 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 119.8/150.0 MB 22.3 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 126.6/150.0 MB 22.7 MB/s eta 0:00:02\n",
      "   ----------------------------------- --- 136.6/150.0 MB 23.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 145.5/150.0 MB 24.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  149.9/150.0 MB 24.6 MB/s eta 0:00:01\n",
      "   --------------------------------------- 150.0/150.0 MB 24.0 MB/s eta 0:00:00\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-3.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T18:48:06.662724Z",
     "iopub.status.busy": "2025-04-14T18:48:06.662445Z",
     "iopub.status.idle": "2025-04-14T20:22:11.003255Z",
     "shell.execute_reply": "2025-04-14T20:22:11.002372Z",
     "shell.execute_reply.started": "2025-04-14T18:48:06.662704Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Baseline Accuracy: 0.14965867320147033\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       1.00      1.00      1.00         1\n",
      "           2       0.00      0.00      0.00         1\n",
      "           3       1.00      1.00      1.00         1\n",
      "           4       0.00      0.00      0.00         1\n",
      "           5       0.00      0.00      0.00         1\n",
      "           6       0.00      0.00      0.00         2\n",
      "           7       0.00      0.00      0.00         1\n",
      "           8       0.00      0.00      0.00         1\n",
      "           9       0.00      0.00      0.00         1\n",
      "          12       0.45      0.33      0.38        15\n",
      "          13       0.50      0.22      0.31         9\n",
      "          14       0.33      0.09      0.14        11\n",
      "          15       0.50      0.22      0.31         9\n",
      "          16       1.00      1.00      1.00         1\n",
      "          17       0.00      0.00      0.00         1\n",
      "          18       0.00      0.00      0.00         3\n",
      "          19       0.00      0.00      0.00        22\n",
      "          20       0.00      0.00      0.00         0\n",
      "          21       1.00      0.67      0.80         3\n",
      "          22       0.00      0.00      0.00         4\n",
      "          25       0.00      0.00      0.00         1\n",
      "          26       0.00      0.00      0.00         0\n",
      "          28       1.00      1.00      1.00         1\n",
      "          29       0.00      0.00      0.00         4\n",
      "          30       0.71      0.83      0.77         6\n",
      "          31       1.00      0.08      0.15        12\n",
      "          32       0.00      0.00      0.00         1\n",
      "          34       0.14      0.14      0.14         7\n",
      "          35       0.00      0.00      0.00         1\n",
      "          36       0.00      0.00      0.00         5\n",
      "          37       0.00      0.00      0.00         1\n",
      "          38       0.00      0.00      0.00         5\n",
      "          40       0.00      0.00      0.00         1\n",
      "          41       1.00      0.33      0.50         3\n",
      "          42       0.50      0.33      0.40         3\n",
      "          43       0.33      0.33      0.33         6\n",
      "          44       0.00      0.00      0.00         1\n",
      "          45       0.57      0.25      0.35        16\n",
      "          46       0.00      0.00      0.00         1\n",
      "          47       0.50      0.25      0.33         4\n",
      "          51       1.00      1.00      1.00         1\n",
      "          52       0.00      0.00      0.00         0\n",
      "          53       0.33      0.33      0.33         3\n",
      "          54       0.00      0.00      0.00         1\n",
      "          55       0.50      0.33      0.40         3\n",
      "          56       0.00      0.00      0.00         1\n",
      "          58       0.50      1.00      0.67         1\n",
      "          59       0.00      0.00      0.00         1\n",
      "          60       0.43      0.17      0.24        18\n",
      "          61       0.14      0.16      0.15        82\n",
      "          62       0.00      0.00      0.00         6\n",
      "          63       0.11      0.06      0.07        18\n",
      "          64       0.00      0.00      0.00        33\n",
      "          65       0.00      0.00      0.00         5\n",
      "          66       0.22      0.42      0.29       122\n",
      "          67       0.00      0.00      0.00        30\n",
      "          68       0.13      0.15      0.14        85\n",
      "          69       0.33      0.16      0.21        19\n",
      "          70       0.05      0.03      0.04        35\n",
      "          71       0.30      0.34      0.32        62\n",
      "          72       0.19      0.17      0.18        76\n",
      "          73       0.25      0.05      0.08        22\n",
      "          74       0.27      0.19      0.22        21\n",
      "          75       0.00      0.00      0.00        11\n",
      "          76       0.43      0.14      0.21        22\n",
      "          77       0.00      0.00      0.00         6\n",
      "          78       0.09      0.07      0.07        46\n",
      "          79       0.00      0.00      0.00        14\n",
      "          80       0.08      0.11      0.09       103\n",
      "          81       0.00      0.00      0.00         8\n",
      "          82       0.00      0.00      0.00        15\n",
      "          83       0.00      0.00      0.00         4\n",
      "          84       0.21      0.30      0.24        76\n",
      "          85       0.00      0.00      0.00         9\n",
      "          86       0.13      0.09      0.11        56\n",
      "          87       0.09      0.10      0.10        60\n",
      "          88       0.00      0.00      0.00        19\n",
      "          89       0.40      0.09      0.14        23\n",
      "          90       0.20      0.15      0.17        54\n",
      "          91       0.22      0.14      0.17        28\n",
      "          92       0.00      0.00      0.00        13\n",
      "          93       0.00      0.00      0.00         8\n",
      "          94       0.09      0.06      0.07        32\n",
      "          95       0.00      0.00      0.00        11\n",
      "          96       0.33      0.14      0.20        29\n",
      "          97       0.15      0.29      0.20       162\n",
      "          98       0.40      0.27      0.32        77\n",
      "          99       0.26      0.17      0.21        52\n",
      "         100       1.00      0.09      0.17        11\n",
      "         101       0.00      0.00      0.00        29\n",
      "         102       0.33      0.10      0.15        10\n",
      "         103       0.00      0.00      0.00        15\n",
      "         104       0.07      0.04      0.05        46\n",
      "         105       0.00      0.00      0.00        21\n",
      "         106       0.00      0.00      0.00        16\n",
      "         107       0.12      0.04      0.06        27\n",
      "         108       0.00      0.00      0.00        17\n",
      "         109       0.00      0.00      0.00        21\n",
      "         110       0.42      0.20      0.27        25\n",
      "         111       0.23      0.26      0.25        68\n",
      "         112       0.33      0.18      0.24        22\n",
      "         113       0.09      0.31      0.14       198\n",
      "         114       0.00      0.00      0.00        13\n",
      "         115       0.11      0.06      0.08        17\n",
      "         116       0.00      0.00      0.00        26\n",
      "         117       0.00      0.00      0.00         5\n",
      "         118       0.00      0.00      0.00        33\n",
      "         119       0.27      0.30      0.28        73\n",
      "         120       0.00      0.00      0.00        13\n",
      "         121       0.19      0.26      0.22        94\n",
      "         122       0.00      0.00      0.00        22\n",
      "         123       0.18      0.14      0.15        66\n",
      "         124       0.15      0.18      0.17        65\n",
      "         125       0.00      0.00      0.00        22\n",
      "         126       0.33      0.11      0.17        18\n",
      "         127       0.00      0.00      0.00         3\n",
      "         128       0.00      0.00      0.00         8\n",
      "         129       0.22      0.16      0.18        44\n",
      "         130       0.00      0.00      0.00        13\n",
      "         131       0.13      0.07      0.09        45\n",
      "         132       0.18      0.10      0.13        30\n",
      "         133       0.00      0.00      0.00        11\n",
      "         134       0.08      0.08      0.08        65\n",
      "         135       0.00      0.00      0.00         4\n",
      "         136       0.15      0.08      0.11        37\n",
      "         137       0.00      0.00      0.00         1\n",
      "         138       0.00      0.00      0.00         8\n",
      "         139       0.00      0.00      0.00        29\n",
      "         140       0.00      0.00      0.00        18\n",
      "         141       0.00      0.00      0.00        11\n",
      "         142       0.00      0.00      0.00        18\n",
      "         143       0.09      0.07      0.08        44\n",
      "         144       0.10      0.21      0.14       142\n",
      "         145       0.00      0.00      0.00         7\n",
      "         146       0.00      0.00      0.00         8\n",
      "         147       0.00      0.00      0.00        30\n",
      "         148       0.00      0.00      0.00        14\n",
      "         149       0.33      0.05      0.09        19\n",
      "         150       0.13      0.07      0.09        28\n",
      "         151       0.07      0.02      0.03        47\n",
      "         152       0.00      0.00      0.00        16\n",
      "         153       0.00      0.00      0.00        52\n",
      "         154       0.00      0.00      0.00         3\n",
      "         155       0.15      0.19      0.17        84\n",
      "         156       0.00      0.00      0.00         3\n",
      "         157       0.00      0.00      0.00        11\n",
      "         158       0.38      0.16      0.22        32\n",
      "         159       0.00      0.00      0.00         3\n",
      "         160       0.20      0.06      0.09        18\n",
      "         161       0.04      0.04      0.04        57\n",
      "         162       0.22      0.09      0.13        22\n",
      "         163       0.07      0.11      0.08        96\n",
      "         164       0.10      0.11      0.11       109\n",
      "         165       0.12      0.06      0.08        47\n",
      "         166       0.27      0.35      0.30        98\n",
      "         167       1.00      0.10      0.18        10\n",
      "         168       0.30      0.15      0.20        40\n",
      "         169       0.22      0.12      0.16        16\n",
      "         170       0.00      0.00      0.00        22\n",
      "         171       0.09      0.07      0.08        42\n",
      "         172       0.12      0.15      0.14        86\n",
      "         173       0.10      0.08      0.09        75\n",
      "         174       0.24      0.15      0.18        34\n",
      "         175       0.33      0.06      0.10        33\n",
      "         176       1.00      0.12      0.22         8\n",
      "         177       0.20      0.03      0.06        31\n",
      "         178       0.12      0.05      0.07        19\n",
      "         179       0.12      0.28      0.17       158\n",
      "         180       0.20      0.21      0.21        80\n",
      "         181       0.18      0.23      0.20        94\n",
      "         182       0.00      0.00      0.00         2\n",
      "         183       0.00      0.00      0.00        14\n",
      "         184       0.08      0.04      0.05        27\n",
      "         185       0.13      0.21      0.16       100\n",
      "         186       0.44      0.19      0.27        21\n",
      "         187       0.09      0.06      0.07        49\n",
      "         188       0.00      0.00      0.00        25\n",
      "         189       0.00      0.00      0.00         7\n",
      "         190       0.16      0.23      0.19       115\n",
      "         191       0.00      0.00      0.00        15\n",
      "         192       0.00      0.00      0.00        10\n",
      "         193       0.00      0.00      0.00         5\n",
      "         194       0.20      0.03      0.05        34\n",
      "         195       0.23      0.17      0.19        53\n",
      "         196       0.00      0.00      0.00        38\n",
      "         197       0.12      0.05      0.08        37\n",
      "         198       0.27      0.21      0.24        47\n",
      "         199       0.00      0.00      0.00        19\n",
      "         200       0.50      0.06      0.11        16\n",
      "         201       0.13      0.06      0.08        48\n",
      "         202       0.00      0.00      0.00        12\n",
      "         203       0.11      0.24      0.15       105\n",
      "         204       0.11      0.10      0.11        60\n",
      "         205       0.25      0.11      0.15        28\n",
      "\n",
      "    accuracy                           0.15      5713\n",
      "   macro avg       0.17      0.12      0.12      5713\n",
      "weighted avg       0.15      0.15      0.14      5713\n",
      "\n",
      "✅ Baseline XGBoost model saved to xgb_baseline_model.pkl\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb_baseline = XGBClassifier(\n",
    "    objective='multi:softprob',\n",
    "    eval_metric='mlogloss',\n",
    "    use_label_encoder=False,\n",
    "    n_jobs=8,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "xgb_baseline.fit(X_train_pca, y_train)\n",
    "\n",
    "y_pred = xgb_baseline.predict(X_test_pca)\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(\"XGBoost Baseline Accuracy:\", acc)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "joblib.dump(xgb_baseline, \"xgb_baseline_model.pkl\")\n",
    "print(\"✅ Baseline XGBoost model saved to xgb_baseline_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-14T20:22:11.003688Z",
     "iopub.status.idle": "2025-04-14T20:22:11.003901Z",
     "shell.execute_reply": "2025-04-14T20:22:11.003809Z",
     "shell.execute_reply.started": "2025-04-14T20:22:11.003799Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
      "✅ Best XGBoost model saved to xgb_best_model.pkl\n",
      "Best XGBoost Accuracy: 0.18134080168037808\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       1.00      1.00      1.00         1\n",
      "           2       0.00      0.00      0.00         1\n",
      "           3       0.50      1.00      0.67         1\n",
      "           4       0.00      0.00      0.00         1\n",
      "           5       0.00      0.00      0.00         1\n",
      "           6       0.00      0.00      0.00         2\n",
      "           7       0.00      0.00      0.00         1\n",
      "           8       0.00      0.00      0.00         1\n",
      "           9       0.00      0.00      0.00         1\n",
      "          11       0.00      0.00      0.00         0\n",
      "          12       0.38      0.40      0.39        15\n",
      "          13       0.33      0.22      0.27         9\n",
      "          14       0.67      0.18      0.29        11\n",
      "          15       0.67      0.22      0.33         9\n",
      "          16       1.00      1.00      1.00         1\n",
      "          17       0.00      0.00      0.00         1\n",
      "          18       0.00      0.00      0.00         3\n",
      "          19       0.00      0.00      0.00        22\n",
      "          21       0.75      1.00      0.86         3\n",
      "          22       0.00      0.00      0.00         4\n",
      "          25       0.00      0.00      0.00         1\n",
      "          26       0.00      0.00      0.00         0\n",
      "          28       1.00      1.00      1.00         1\n",
      "          29       0.00      0.00      0.00         4\n",
      "          30       0.71      0.83      0.77         6\n",
      "          31       0.67      0.17      0.27        12\n",
      "          32       0.00      0.00      0.00         1\n",
      "          34       0.20      0.29      0.24         7\n",
      "          35       0.00      0.00      0.00         1\n",
      "          36       0.00      0.00      0.00         5\n",
      "          37       0.00      0.00      0.00         1\n",
      "          38       0.00      0.00      0.00         5\n",
      "          39       0.00      0.00      0.00         0\n",
      "          40       0.00      0.00      0.00         1\n",
      "          41       0.67      0.67      0.67         3\n",
      "          42       0.33      0.33      0.33         3\n",
      "          43       0.25      0.17      0.20         6\n",
      "          44       0.00      0.00      0.00         1\n",
      "          45       0.22      0.12      0.16        16\n",
      "          46       0.00      0.00      0.00         1\n",
      "          47       0.50      0.25      0.33         4\n",
      "          51       0.50      1.00      0.67         1\n",
      "          52       0.00      0.00      0.00         0\n",
      "          53       0.50      0.33      0.40         3\n",
      "          54       0.00      0.00      0.00         1\n",
      "          55       0.40      0.67      0.50         3\n",
      "          56       0.00      0.00      0.00         1\n",
      "          58       0.50      1.00      0.67         1\n",
      "          59       0.00      0.00      0.00         1\n",
      "          60       0.80      0.22      0.35        18\n",
      "          61       0.20      0.29      0.24        82\n",
      "          62       0.00      0.00      0.00         6\n",
      "          63       0.67      0.11      0.19        18\n",
      "          64       0.00      0.00      0.00        33\n",
      "          65       0.00      0.00      0.00         5\n",
      "          66       0.26      0.55      0.35       122\n",
      "          67       0.14      0.03      0.05        30\n",
      "          68       0.13      0.15      0.14        85\n",
      "          69       0.18      0.11      0.13        19\n",
      "          70       0.08      0.03      0.04        35\n",
      "          71       0.37      0.40      0.39        62\n",
      "          72       0.21      0.21      0.21        76\n",
      "          73       0.00      0.00      0.00        22\n",
      "          74       0.38      0.38      0.38        21\n",
      "          75       0.00      0.00      0.00        11\n",
      "          76       0.00      0.00      0.00        22\n",
      "          77       0.00      0.00      0.00         6\n",
      "          78       0.14      0.13      0.13        46\n",
      "          79       0.00      0.00      0.00        14\n",
      "          80       0.12      0.15      0.13       103\n",
      "          81       0.00      0.00      0.00         8\n",
      "          82       0.00      0.00      0.00        15\n",
      "          83       0.00      0.00      0.00         4\n",
      "          84       0.22      0.32      0.26        76\n",
      "          85       0.00      0.00      0.00         9\n",
      "          86       0.15      0.14      0.15        56\n",
      "          87       0.13      0.12      0.12        60\n",
      "          88       0.00      0.00      0.00        19\n",
      "          89       0.14      0.04      0.07        23\n",
      "          90       0.17      0.19      0.18        54\n",
      "          91       0.24      0.18      0.20        28\n",
      "          92       0.00      0.00      0.00        13\n",
      "          93       0.00      0.00      0.00         8\n",
      "          94       0.08      0.06      0.07        32\n",
      "          95       0.00      0.00      0.00        11\n",
      "          96       0.35      0.21      0.26        29\n",
      "          97       0.16      0.33      0.22       162\n",
      "          98       0.44      0.35      0.39        77\n",
      "          99       0.31      0.19      0.24        52\n",
      "         100       1.00      0.09      0.17        11\n",
      "         101       0.00      0.00      0.00        29\n",
      "         102       0.50      0.10      0.17        10\n",
      "         103       0.00      0.00      0.00        15\n",
      "         104       0.14      0.07      0.09        46\n",
      "         105       0.25      0.10      0.14        21\n",
      "         106       0.00      0.00      0.00        16\n",
      "         107       0.15      0.07      0.10        27\n",
      "         108       0.00      0.00      0.00        17\n",
      "         109       0.17      0.05      0.07        21\n",
      "         110       0.14      0.08      0.10        25\n",
      "         111       0.24      0.31      0.27        68\n",
      "         112       0.27      0.18      0.22        22\n",
      "         113       0.14      0.39      0.20       198\n",
      "         114       0.20      0.08      0.11        13\n",
      "         115       0.17      0.06      0.09        17\n",
      "         116       0.00      0.00      0.00        26\n",
      "         117       1.00      0.20      0.33         5\n",
      "         118       0.00      0.00      0.00        33\n",
      "         119       0.28      0.37      0.32        73\n",
      "         120       0.50      0.08      0.13        13\n",
      "         121       0.20      0.28      0.23        94\n",
      "         122       0.00      0.00      0.00        22\n",
      "         123       0.21      0.11      0.14        66\n",
      "         124       0.21      0.26      0.23        65\n",
      "         125       0.00      0.00      0.00        22\n",
      "         126       0.40      0.11      0.17        18\n",
      "         127       0.00      0.00      0.00         3\n",
      "         128       0.00      0.00      0.00         8\n",
      "         129       0.27      0.18      0.22        44\n",
      "         130       0.33      0.08      0.12        13\n",
      "         131       0.17      0.07      0.10        45\n",
      "         132       0.19      0.10      0.13        30\n",
      "         133       0.00      0.00      0.00        11\n",
      "         134       0.12      0.12      0.12        65\n",
      "         135       0.00      0.00      0.00         4\n",
      "         136       0.13      0.05      0.08        37\n",
      "         137       0.00      0.00      0.00         1\n",
      "         138       0.00      0.00      0.00         8\n",
      "         139       0.00      0.00      0.00        29\n",
      "         140       0.00      0.00      0.00        18\n",
      "         141       0.25      0.09      0.13        11\n",
      "         142       0.00      0.00      0.00        18\n",
      "         143       0.27      0.16      0.20        44\n",
      "         144       0.11      0.20      0.15       142\n",
      "         145       0.00      0.00      0.00         7\n",
      "         146       0.00      0.00      0.00         8\n",
      "         147       0.30      0.10      0.15        30\n",
      "         148       0.00      0.00      0.00        14\n",
      "         149       0.25      0.05      0.09        19\n",
      "         150       0.07      0.04      0.05        28\n",
      "         151       0.05      0.02      0.03        47\n",
      "         152       0.00      0.00      0.00        16\n",
      "         153       0.03      0.02      0.02        52\n",
      "         154       0.00      0.00      0.00         3\n",
      "         155       0.09      0.12      0.10        84\n",
      "         156       0.00      0.00      0.00         3\n",
      "         157       0.00      0.00      0.00        11\n",
      "         158       0.29      0.22      0.25        32\n",
      "         159       0.00      0.00      0.00         3\n",
      "         160       0.00      0.00      0.00        18\n",
      "         161       0.06      0.04      0.05        57\n",
      "         162       0.33      0.14      0.19        22\n",
      "         163       0.11      0.14      0.12        96\n",
      "         164       0.11      0.08      0.09       109\n",
      "         165       0.04      0.02      0.03        47\n",
      "         166       0.29      0.38      0.33        98\n",
      "         167       1.00      0.10      0.18        10\n",
      "         168       0.26      0.12      0.17        40\n",
      "         169       0.22      0.12      0.16        16\n",
      "         170       0.00      0.00      0.00        22\n",
      "         171       0.19      0.24      0.21        42\n",
      "         172       0.21      0.26      0.23        86\n",
      "         173       0.11      0.08      0.09        75\n",
      "         174       0.31      0.15      0.20        34\n",
      "         175       0.25      0.06      0.10        33\n",
      "         176       1.00      0.12      0.22         8\n",
      "         177       0.00      0.00      0.00        31\n",
      "         178       0.17      0.05      0.08        19\n",
      "         179       0.18      0.37      0.24       158\n",
      "         180       0.27      0.31      0.29        80\n",
      "         181       0.22      0.27      0.24        94\n",
      "         182       0.00      0.00      0.00         2\n",
      "         183       0.00      0.00      0.00        14\n",
      "         184       0.00      0.00      0.00        27\n",
      "         185       0.12      0.19      0.15       100\n",
      "         186       1.00      0.29      0.44        21\n",
      "         187       0.12      0.10      0.11        49\n",
      "         188       0.18      0.08      0.11        25\n",
      "         189       0.00      0.00      0.00         7\n",
      "         190       0.17      0.30      0.22       115\n",
      "         191       1.00      0.07      0.12        15\n",
      "         192       0.00      0.00      0.00        10\n",
      "         193       0.00      0.00      0.00         5\n",
      "         194       0.18      0.06      0.09        34\n",
      "         195       0.25      0.23      0.24        53\n",
      "         196       0.00      0.00      0.00        38\n",
      "         197       0.22      0.14      0.17        37\n",
      "         198       0.33      0.28      0.30        47\n",
      "         199       0.14      0.05      0.08        19\n",
      "         200       0.00      0.00      0.00        16\n",
      "         201       0.14      0.08      0.11        48\n",
      "         202       0.00      0.00      0.00        12\n",
      "         203       0.15      0.30      0.20       105\n",
      "         204       0.07      0.07      0.07        60\n",
      "         205       0.26      0.21      0.24        28\n",
      "\n",
      "    accuracy                           0.18      5713\n",
      "   macro avg       0.19      0.14      0.14      5713\n",
      "weighted avg       0.18      0.18      0.16      5713\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "param_grid = {\n",
    "    'max_depth': [3, 5, 7, 10],\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0]\n",
    "}\n",
    "\n",
    "xgb = XGBClassifier(\n",
    "    objective='multi:softprob',\n",
    "    num_class=len(np.unique(y_train)),\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='mlogloss',\n",
    "    n_jobs=2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=xgb,\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=20,\n",
    "    scoring='accuracy',\n",
    "    cv=cv,\n",
    "    verbose=2,\n",
    "    random_state=42,\n",
    "    n_jobs=2\n",
    ")\n",
    "\n",
    "random_search.fit(X_train_pca, y_train)\n",
    "\n",
    "best_model = random_search.best_estimator_\n",
    "joblib.dump(best_model, \"xgb_best_model.pkl\")\n",
    "print(\"✅ Best XGBoost model saved to xgb_best_model.pkl\")\n",
    "\n",
    "y_pred = best_model.predict(X_test_pca) # type: ignore\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(\"Best XGBoost Accuracy:\", acc)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Accuracy: 0.12042709609662174\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "Best Params: {'solver': 'saga', 'penalty': 'l1', 'C': 0.3039195382313198}\n",
      "Model saved to 'best_logistic_regression.pkl'\n",
      "Final Accuracy after tuning: 0.11990197794503764\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "baseline_lr = LogisticRegression(\n",
    "    multi_class='multinomial',\n",
    "    solver='saga',\n",
    "    penalty='l2',\n",
    "    max_iter=500,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "baseline_lr.fit(X_train_pca, y_train)\n",
    "y_pred_base = baseline_lr.predict(X_test_pca)\n",
    "print(\"Baseline Accuracy:\", accuracy_score(y_test, y_pred_base))\n",
    "\n",
    "param_grid = {\n",
    "    'C': np.logspace(-3, 1, 30),\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'solver': ['saga']\n",
    "}\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=baseline_lr,\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=20,\n",
    "    scoring='accuracy',\n",
    "    cv=skf,\n",
    "    verbose=1,\n",
    "    random_state=42,\n",
    "    n_jobs=2\n",
    ")\n",
    "\n",
    "random_search.fit(X_train_pca, y_train)\n",
    "\n",
    "best_lr = random_search.best_estimator_\n",
    "print(\"Best Params:\", random_search.best_params_)\n",
    "\n",
    "joblib.dump(best_lr, 'best_logistic_regression.pkl')\n",
    "print(\"Model saved to 'best_logistic_regression.pkl'\")\n",
    "\n",
    "y_pred_final = best_lr.predict(X_test_pca) # type: ignore\n",
    "acc_final = accuracy_score(y_test, y_pred_final)\n",
    "print(\"Final Accuracy after tuning:\", acc_final)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 11361821,
     "sourceId": 91844,
     "sourceType": "competition"
    },
    {
     "datasetId": 7144075,
     "sourceId": 11405342,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
