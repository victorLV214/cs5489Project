{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bird Project Final (In Kaggle Env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T12:49:23.247346Z",
     "iopub.status.busy": "2025-04-17T12:49:23.245813Z",
     "iopub.status.idle": "2025-04-17T12:49:23.255345Z",
     "shell.execute_reply": "2025-04-17T12:49:23.254102Z",
     "shell.execute_reply.started": "2025-04-17T12:49:23.247305Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import glob\n",
    "import logging\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "import numpy as np\n",
    "import pandas\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "import torchaudio.transforms as AT\n",
    "from scipy import signal\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from tensorflow.keras.models import load_model\n",
    "from tqdm import tqdm\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "torch.set_num_threads(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T12:49:30.796625Z",
     "iopub.status.busy": "2025-04-17T12:49:30.796265Z",
     "iopub.status.idle": "2025-04-17T12:49:30.820950Z",
     "shell.execute_reply": "2025-04-17T12:49:30.819852Z",
     "shell.execute_reply.started": "2025-04-17T12:49:30.796599Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class MyAudioConfig:\n",
    "    def __init__(self):\n",
    "\n",
    "        self.sample_rate = 32000\n",
    "        # Audio window length, in seconds\n",
    "        self.window_size = 5\n",
    "\n",
    "        # Mel spectrum parameters\n",
    "        self.n_fft = 2048  # FFT window size (frequency domain resolution)\n",
    "        self.hop_length = (\n",
    "            512  # How many points are used to perform a window (time resolution)\n",
    "        )\n",
    "        self.n_mels = 128  # The number of Mel bins output, the larger the number, the higher the frequency resolution\n",
    "        self.fmin = 20  # Minimum frequency range\n",
    "        self.fmax = 16000  # Maximum frequency range (half the sampling rate)\n",
    "\n",
    "        self.power = 2.0  # Power spectrum index, 2.0 represents power spectrum, 1.0 represents amplitude spectrum\n",
    "\n",
    "        self.target_shape = (256, 128)\n",
    "\n",
    "        # Noise processing parameters\n",
    "        self.apply_noise_reduction = True  # Whether to enable noise reduction\n",
    "        self.noise_reduction_strength = 0.1  # Noise reduction strength: the ratio of the original signal + the noise reduction signal\n",
    "\n",
    "        # Audio normalization, remove DC offset and normalize amplitude\n",
    "        self.apply_normalization = True\n",
    "\n",
    "        # Spectrum contrast enhancement\n",
    "        self.apply_spec_contrast = True\n",
    "        self.contrast_factor = 0.15\n",
    "\n",
    "        # SpecAugment\n",
    "        self.use_spec_augment = False\n",
    "        self.freq_mask_param = 20\n",
    "        self.time_mask_param = 30\n",
    "        self.freq_mask_count = 1\n",
    "        self.time_mask_count = 1\n",
    "\n",
    "        # Small constant for numerical stability\n",
    "        self.eps = 1e-6\n",
    "\n",
    "\n",
    "class MyAudioPipeline:\n",
    "    \"\"\"\n",
    "    A pipeline class that converts raw audio waveform into\n",
    "    a final Mel-spectrogram, mirroring the structure of the\n",
    "    original big code. This includes:\n",
    "    - Noise reduction\n",
    "    - Normalization\n",
    "    - Mel-spectrogram\n",
    "    - dB scaling + minmax normalization\n",
    "    - (Optional) Contrast enhancement\n",
    "    - (Optional) SpecAugment\n",
    "    - Resizing to target shape\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cfg: MyAudioConfig):\n",
    "        self.cfg = cfg\n",
    "\n",
    "    def reduce_noise(self, audio_data: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Apply simple median-filter-based denoising, then mix\n",
    "        original signal and denoised signal by noise_reduction_strength.\n",
    "        \"\"\"\n",
    "        if not self.cfg.apply_noise_reduction:\n",
    "            return audio_data\n",
    "\n",
    "        # Median filter\n",
    "        window_size = 5\n",
    "        audio_denoised = signal.medfilt(audio_data, window_size)\n",
    "\n",
    "        # Mix\n",
    "        alpha = self.cfg.noise_reduction_strength\n",
    "        return (1 - alpha) * audio_data + alpha * audio_denoised\n",
    "\n",
    "    def normalize_audio(self, audio_data: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Remove DC offset and scale to [-1, 1].\n",
    "        \"\"\"\n",
    "        if not self.cfg.apply_normalization:\n",
    "            return audio_data\n",
    "\n",
    "        mean_val = np.mean(audio_data)\n",
    "        audio_data = audio_data - mean_val\n",
    "\n",
    "        max_amp = np.max(np.abs(audio_data))\n",
    "        if max_amp > 0:\n",
    "            audio_data = audio_data / max_amp\n",
    "\n",
    "        return audio_data\n",
    "\n",
    "    def enhance_spectrogram_contrast(\n",
    "        self, spec: np.ndarray, factor: float = 0.15\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Enhance spectrogram contrast. We shift values away from the mean,\n",
    "        then clip to [0,1].\n",
    "        \"\"\"\n",
    "        mean_val = np.mean(spec)\n",
    "        enhanced = mean_val + (spec - mean_val) * (1 + factor)\n",
    "        return np.clip(enhanced, 0, 1)\n",
    "\n",
    "    def apply_spec_augment(self, spec: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        SpecAugment: randomly mask along frequency & time axes.\n",
    "        \"\"\"\n",
    "        if not self.cfg.use_spec_augment:\n",
    "            return spec\n",
    "\n",
    "        augmented = spec.copy()\n",
    "\n",
    "        # Frequency mask\n",
    "        for _ in range(self.cfg.freq_mask_count):\n",
    "            f = np.random.randint(0, self.cfg.freq_mask_param)\n",
    "            f0 = np.random.randint(0, augmented.shape[0] - f)\n",
    "            augmented[f0 : f0 + f, :] = 0\n",
    "\n",
    "        # Time mask\n",
    "        for _ in range(self.cfg.time_mask_count):\n",
    "            t = np.random.randint(0, self.cfg.time_mask_param)\n",
    "            t0 = np.random.randint(0, augmented.shape[1] - t)\n",
    "            augmented[:, t0 : t0 + t] = 0\n",
    "\n",
    "        return augmented\n",
    "\n",
    "    def audio_to_melspec(self, audio_data: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Core function that:\n",
    "         1) Pad or trim to self.cfg.window_size\n",
    "         2) reduce_noise -> normalize_audio\n",
    "         3) librosa.feature.melspectrogram\n",
    "         4) power_to_db -> [0,1] minmax\n",
    "         5) (Optional) enhance contrast\n",
    "         6) (Optional) spec augment\n",
    "         7) resize\n",
    "        \"\"\"\n",
    "        # 1) Pad or trim to exactly (sample_rate * window_size) samples\n",
    "        required_samples = self.cfg.sample_rate * self.cfg.window_size\n",
    "        if len(audio_data) < required_samples:\n",
    "            audio_data = np.pad(\n",
    "                audio_data, (0, required_samples - len(audio_data)), mode=\"constant\"\n",
    "            )\n",
    "        elif len(audio_data) > required_samples:\n",
    "            audio_data = audio_data[:required_samples]\n",
    "\n",
    "        # 2) Denoise & Normalize\n",
    "        audio_data = self.reduce_noise(audio_data)\n",
    "        audio_data = self.normalize_audio(audio_data)\n",
    "\n",
    "        # 3) Mel-spectrogram\n",
    "        mel = librosa.feature.melspectrogram(\n",
    "            y=audio_data,\n",
    "            sr=self.cfg.sample_rate,\n",
    "            n_fft=self.cfg.n_fft,\n",
    "            hop_length=self.cfg.hop_length,\n",
    "            n_mels=self.cfg.n_mels,\n",
    "            fmin=self.cfg.fmin,\n",
    "            fmax=self.cfg.fmax,\n",
    "            power=self.cfg.power,\n",
    "        )\n",
    "\n",
    "        # 4) Convert to dB, then min-max to [0,1]\n",
    "        mel_db = librosa.power_to_db(mel, ref=np.max)\n",
    "        db_min, db_max = mel_db.min(), mel_db.max()\n",
    "        mel_norm = (mel_db - db_min) / (db_max - db_min + self.cfg.eps)\n",
    "\n",
    "        # 5) Enhance contrast if needed\n",
    "        if self.cfg.apply_spec_contrast:\n",
    "            mel_norm = self.enhance_spectrogram_contrast(\n",
    "                mel_norm, self.cfg.contrast_factor\n",
    "            )\n",
    "\n",
    "        # 6) SpecAugment (time/freq mask)\n",
    "        mel_aug = self.apply_spec_augment(mel_norm)\n",
    "\n",
    "        # 7) Resize\n",
    "        # if target_shape=(256,128) means (width=256, height=128)\n",
    "        # but mel shape is (n_mels, time_frames) -> (128, ???)\n",
    "        # So we do: cv2.resize(mel_aug, (width, height))\n",
    "        if self.cfg.target_shape is not None:\n",
    "            mel_aug = cv2.resize(\n",
    "                mel_aug, self.cfg.target_shape, interpolation=cv2.INTER_LINEAR\n",
    "            )\n",
    "\n",
    "        # Return float32 array\n",
    "        return mel_aug.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T12:49:36.210264Z",
     "iopub.status.busy": "2025-04-17T12:49:36.209924Z",
     "iopub.status.idle": "2025-04-17T12:53:18.499364Z",
     "shell.execute_reply": "2025-04-17T12:53:18.497471Z",
     "shell.execute_reply.started": "2025-04-17T12:49:36.210242Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-17 12:49:36.406321: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of test audio files found： 9726\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6e66c747211485bbca4ede60d799e03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9726 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_31/2103647952.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mmel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m          \u001b[0;31m# (1,128,256,1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m        \u001b[0;31m# (206,)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mrow_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{base}_{(k+1)*WIN_SEC}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks)\u001b[0m\n\u001b[1;32m    507\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_stop_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 509\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menumerate_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    510\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_predict_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36menumerate_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    687\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_current_iterator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 689\u001b[0;31m             \u001b[0miterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_distributed_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    690\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_batches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m                 for step in range(\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    499\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minside_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 501\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0miterator_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOwnedIterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    502\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m       raise RuntimeError(\"`tf.data.Dataset` only supports Python-style \"\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[1;32m    707\u001b[0m             \u001b[0;34m\"When `dataset` is provided, `element_spec` and `components` must \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    708\u001b[0m             \"not be specified.\")\n\u001b[0;32m--> 709\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    710\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_next_call_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_create_iterator\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    746\u001b[0m             self._flat_output_types)\n\u001b[1;32m    747\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_set_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfulltype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 748\u001b[0;31m       \u001b[0mgen_dataset_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds_variant\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    749\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36mmake_iterator\u001b[0;34m(dataset, iterator, name)\u001b[0m\n\u001b[1;32m   3476\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3477\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3478\u001b[0;31m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[1;32m   3479\u001b[0m         _ctx, \"MakeIterator\", name, dataset, iterator)\n\u001b[1;32m   3480\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Read sample_submission.csv to get the column order\n",
    "sample = pd.read_csv(\"/kaggle/input/birdclef-2025/sample_submission.csv\")\n",
    "species_columns = list(sample.columns[1:])\n",
    "NUM_SPECIES = len(species_columns)\n",
    "\n",
    "model = tf.keras.models.load_model(\"/kaggle/input/my_resnet_model_improved/keras/default/1/my_resnet_model_improved.h5\")\n",
    "\n",
    "cfg = MyAudioConfig()\n",
    "pipeline = MyAudioPipeline(cfg)\n",
    "TEST_DIR = \"/kaggle/input/birdclef-2025/test_soundscapes\"\n",
    "WIN_SEC  = cfg.window_size\n",
    "WIN_SAMPLES = cfg.sample_rate * WIN_SEC\n",
    "\n",
    "rows = []\n",
    "ogg_list = sorted(glob.glob(os.path.join(TEST_DIR, \"*.ogg\")))\n",
    "print(\"Total number of test audio files found：\", len(ogg_list))\n",
    "\n",
    "for ogg in tqdm(ogg_list):\n",
    "    y, _ = librosa.load(ogg, sr=cfg.sample_rate, mono=True)\n",
    "    n_windows = len(y) // WIN_SAMPLES\n",
    "\n",
    "    base = os.path.splitext(os.path.basename(ogg))[0]\n",
    "\n",
    "    for k in range(n_windows):\n",
    "        seg = y[k*WIN_SAMPLES : (k+1)*WIN_SAMPLES]\n",
    "        mel = pipeline.audio_to_melspec(seg)            # (128,256)\n",
    "        mel = np.expand_dims(mel, axis=(0,-1))          # (1,128,256,1)\n",
    "\n",
    "        probs = model.predict(mel, verbose=0)[0]        # (206,)\n",
    "\n",
    "        row_id = f\"{base}_{(k+1)*WIN_SEC}\"\n",
    "        rows.append([row_id, *probs])\n",
    "        \n",
    "\n",
    "sub_df = pd.DataFrame(rows, columns=[\"row_id\"] + species_columns)\n",
    "sub_df.to_csv(\"submission.csv\", index=False)\n",
    "print(\"submission.csv saved\", sub_df.shape)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 11361821,
     "isSourceIdPinned": false,
     "sourceId": 91844,
     "sourceType": "competition"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 307313,
     "modelInstanceId": 286485,
     "sourceId": 342462,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
